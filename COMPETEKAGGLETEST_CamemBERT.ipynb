{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Training a Camembert Model for Text Difficulty Classification**"
      ],
      "metadata": {
        "id": "luZiIVvqTl4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the necessary import, including sentencepiece that is mandatory for a camembert model (sometimes after installation, we must relaunch the environment)"
      ],
      "metadata": {
        "id": "aBZ6PwguT-tC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU5a2kcpfkK9",
        "outputId": "201adf29-88c4-4001-eaa5-efec99b76b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Collecting PyTorch\n",
            "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: PyTorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for PyTorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for PyTorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for PyTorch\n",
            "Failed to build PyTorch\n",
            "\u001b[31mERROR: Could not build wheels for PyTorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: TensorFlow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->TensorFlow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->TensorFlow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->TensorFlow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->TensorFlow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->TensorFlow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->TensorFlow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->TensorFlow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->TensorFlow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->TensorFlow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->TensorFlow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->TensorFlow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->TensorFlow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->TensorFlow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->TensorFlow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->TensorFlow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->TensorFlow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->TensorFlow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->TensorFlow) (3.2.2)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install PyTorch\n",
        "!pip install TensorFlow\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup, CamembertTokenizer\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import huggingface_hub"
      ],
      "metadata": {
        "id": "FSAhbb2Dgp2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "7kDos6uoj14j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the data from my GIT repo"
      ],
      "metadata": {
        "id": "OdBOomcuUUin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "\n",
        "!git clone https://github.com/melvin2504/Detecting-the-difficulty-level-of-French-texts/ project_repo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m95RHm0Agr7h",
        "outputId": "77796c47-f2a0-4fe2-dcff-00d5f5ed26e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'project_repo'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 77 (delta 6), reused 1 (delta 1), pack-reused 66\u001b[K\n",
            "Receiving objects: 100% (77/77), 1.51 MiB | 10.75 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/project_repo/data/training_data.csv\")\n",
        "unlabeled_df = pd.read_csv(\"/content/project_repo/data/unlabelled_test_data.csv\")"
      ],
      "metadata": {
        "id": "b2M1lpXYguxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing and Model Initialization**\n",
        "# Step 1: Initialize the Label Encoder"
      ],
      "metadata": {
        "id": "qAxs2RT-VsW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode the difficulty labels\n",
        "df['difficulty_encoded'] = label_encoder.fit_transform(df['difficulty'])\n"
      ],
      "metadata": {
        "id": "xLs6yaQxlz7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Split the Dataset\n",
        "\n",
        "Here, we split the dataset into training and testing sets. The test set is 20% of the entire dataset. That is a parameters we can change to fine-tune the model"
      ],
      "metadata": {
        "id": "CQTgCbWoV1gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "3GFjz22Ojixu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Compute Class Weights\n",
        "\n",
        "This section calculates the class weights, which helps in dealing with imbalanced datasets by assigning different weights to different classes (when we augment the data for specific labels for example)"
      ],
      "metadata": {
        "id": "Z4A8Uf01WH9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(df['difficulty_encoded']), y=df['difficulty_encoded'])\n",
        "\n",
        "# Convert class_weights to a tensor\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "class_weights = class_weights.to('cuda')  # If you are using a GPU"
      ],
      "metadata": {
        "id": "GP2_oeMuWsEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Define Loss Function\n",
        "\n",
        "The CrossEntropyLoss function is defined with the computed class weights. This is particularly useful for classification tasks."
      ],
      "metadata": {
        "id": "lG2SPk4QWrPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function with class weights\n",
        "loss_fn = CrossEntropyLoss(weight=class_weights)"
      ],
      "metadata": {
        "id": "SDXuZtsoWsuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Create a Custom Dataset Class\n",
        "\n",
        "We create a custom dataset class for handling French text. This includes methods for initializing the dataset, calculating its length, and retrieving a specific item.\n",
        "\n",
        "#### Class Definition\n",
        "- **Purpose**: The `FrenchTextDataset` class, inheriting from PyTorch's `Dataset`, is tailored for handling French texts and their corresponding labels. It's specifically designed for our text classification task.\n",
        "\n",
        "#### Initialization Method (`__init__`)\n",
        "- **Functionality**: Initializes the dataset with texts, labels, a tokenizer, and a maximum token length. These parameters are essential for preparing the dataset for the model.\n",
        "- **Parameters**:\n",
        "  - `texts`: The French texts to be used for training/testing.\n",
        "  - `labels`: Corresponding labels for each text.\n",
        "  - `tokenizer`: A tokenizer for text processing, specific to our model's needs.\n",
        "  - `max_token_len`: The maximum length for tokenization, ensuring uniformity in input size.\n",
        "\n",
        "#### Length Method (`__len__`)\n",
        "- **Purpose**: Returns the total number of texts in the dataset. This method is crucial for PyTorch to understand the size of the dataset.\n",
        "\n",
        "#### Get Item Method (`__getitem__`)\n",
        "- **Functionality**: Retrieves a specific item (text and label) from the dataset, based on an index. This method is called during dataset iteration.\n",
        "- **Process**:\n",
        "  - Retrieves the text and its corresponding label.\n",
        "  - Uses the `encode_plus` method of the tokenizer to process the text:\n",
        "    - Adds special tokens (like [CLS], [SEP]).\n",
        "    - Applies padding or truncation to fit `max_token_len`.\n",
        "    - Generates an attention mask.\n",
        "  - Returns a dictionary containing `input_ids`, `attention_mask`, and `labels`, essential for model training in PyTorch.\n"
      ],
      "metadata": {
        "id": "HpGWvMF6WxAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FrenchTextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_token_len=512):\n",
        "        # Store the texts, labels, tokenizer, and maximum token length as attributes\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_token_len = max_token_len\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of texts in the dataset\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Get the text and label at the specified index\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Tokenize the text with added special tokens, padding, and truncation\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,  # Add [CLS] and [SEP] tokens\n",
        "            max_length=self.max_token_len,  # Truncate/pad to this length\n",
        "            return_token_type_ids=False,  # Don't return token type ids\n",
        "            padding='max_length',  # Pad to max_length\n",
        "            truncation=True,  # Truncate to max_length\n",
        "            return_attention_mask=True,  # Return attention mask\n",
        "            return_tensors='pt'  # Return PyTorch tensors\n",
        "        )\n",
        "\n",
        "        # Return a dictionary with input_ids, attention_mask, and labels\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),  # Flatten the input_ids tensor\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),  # Flatten the attention mask tensor\n",
        "            'labels': torch.tensor(label, dtype=torch.long)  # Convert label to a long tensor\n",
        "        }"
      ],
      "metadata": {
        "id": "2nJ0vCBjjtra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Initialize CamemBERT Tokenizer\n",
        "\n",
        "Initializes the Camembert tokenizer, which is essential for tokenizing French text."
      ],
      "metadata": {
        "id": "XzY05WXiW9s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize CamemBERT tokenizer\n",
        "# Here, we are initializing the tokenizer specific to the CamemBERT model.\n",
        "# 'camembert/camembert-base-ccnet' is the pre-trained model identifier.\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert/camembert-base-ccnet')\n",
        "\n",
        "# Create the training dataset\n",
        "# The train_dataset is created by passing the training sentences and their corresponding encoded difficulty labels.\n",
        "# The tokenizer defined above is used for processing the texts in the dataset.\n",
        "# This dataset will be used for training the model.\n",
        "train_dataset = FrenchTextDataset(\n",
        "    texts=train_df['sentence'].to_numpy(),  # Convert the sentence column to numpy array for processing\n",
        "    labels=train_df['difficulty_encoded'].to_numpy(),  # Convert the encoded labels to numpy array\n",
        "    tokenizer=tokenizer  # Use the initialized CamembertTokenizer\n",
        ")\n",
        "\n",
        "# Create the testing dataset\n",
        "# Similarly to train_dataset, test_dataset is created for evaluating the model.\n",
        "# It uses the test sentences and their labels, ensuring that the model can be evaluated on unseen data.\n",
        "test_dataset = FrenchTextDataset(\n",
        "    texts=test_df['sentence'].to_numpy(),  # Convert the sentence column of test data to numpy array\n",
        "    labels=test_df['difficulty_encoded'].to_numpy(),  # Convert the encoded labels of test data to numpy array\n",
        "    tokenizer=tokenizer  # Use the same tokenizer as for the training dataset\n",
        ")\n"
      ],
      "metadata": {
        "id": "g_jgVKoqj6Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Create Data Loaders\n",
        "Data loaders are created for both the training and testing datasets. These are used to feed data into the model in batches."
      ],
      "metadata": {
        "id": "G3iD_nXVXFCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "hymsGzsDl-66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Initialize the CamemBERT Model\n",
        "Finally, we initialize the CamemBERT model for sequence classification. The number of labels is set based on the unique values in the difficulty column."
      ],
      "metadata": {
        "id": "Z1vy44OeXL4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CamembertForSequenceClassification"
      ],
      "metadata": {
        "id": "EelAnJy3kHs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CamembertForSequenceClassification.from_pretrained(\n",
        "    'camembert/camembert-base-ccnet',\n",
        "    num_labels=df['difficulty'].nunique()  # Adjust based on your number of classes\n",
        ")\n",
        "model = model.to('cuda')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOsIC8abj9PA",
        "outputId": "356d9f8a-ba87-4ae1-f778-51e9eb9360b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Training and Evaluation Loop\n",
        "\n",
        "In this section, we perform the training and evaluation of our CamemBERT model. We use the AdamW optimizer with a learning rate of `5e-5`. The training process involves iterating over our training data, performing a forward pass, computing the loss, and updating the model parameters through backpropagation.\n",
        "\n",
        "We also define an `evaluate` function, which calculates the loss on the validation dataset. This function is crucial for monitoring the model's performance on unseen data and helps in preventing overfitting.\n",
        "\n",
        "After each training epoch, we evaluate the model on the validation set. If the validation loss does not improve after a certain number of epochs (defined by the `patience` parameter), we stop the training early. This approach of early stopping is an effective regularization technique to combat overfitting.\n"
      ],
      "metadata": {
        "id": "UjphqL-xYoK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW"
      ],
      "metadata": {
        "id": "mLsXlI5-mVBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the optimizer with model parameters and learning rate\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Define the evaluation function for the validation set\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0  # Initialize validation loss\n",
        "\n",
        "    # Disable gradient calculations for validation to save memory and computations\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:  # Iterate over each batch in the validation loader\n",
        "            # Move input and labels to GPU for faster processing\n",
        "            input_ids = batch['input_ids'].to('cuda')\n",
        "            attention_mask = batch['attention_mask'].to('cuda')\n",
        "            labels = batch['labels'].to('cuda')\n",
        "\n",
        "            # Forward pass without gradient calculation\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()  # Accumulate the loss\n",
        "\n",
        "    return val_loss / len(val_loader)  # Calculate average validation loss\n",
        "\n",
        "# Initialize variables for early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience = 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(4):  # Iterate over epochs\n",
        "    model.train()  # Set model to training mode\n",
        "    for batch in train_loader:  # Iterate over each batch in the training loader\n",
        "        # Move input and labels to GPU\n",
        "        input_ids = batch['input_ids'].to('cuda')\n",
        "        attention_mask = batch['attention_mask'].to('cuda')\n",
        "        labels = batch['labels'].to('cuda')\n",
        "\n",
        "        # Forward pass and compute loss\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = loss_fn(outputs.logits, labels)  # Calculate loss\n",
        "        loss.backward()  # Backpropagation\n",
        "\n",
        "        optimizer.step()  # Update model parameters\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "    # Evaluation after each epoch\n",
        "    val_loss = evaluate(model, test_loader)\n",
        "    print(f\"Epoch {epoch} completed. Validation Loss: {val_loss}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss  # Update the best validation loss\n",
        "        patience = 0  # Reset patience\n",
        "    else:\n",
        "        patience += 1\n",
        "        if patience > 1:  # Stop training if no improvement after certain epochs (can change the patience before breaking)\n",
        "            print(\"Stopping early due to overfitting.\")\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfxR6xoOkMTq",
        "outputId": "f7e74b5d-890b-41a2-cb70-7c3df49295ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 completed. Validation Loss: 1.2656187097231546\n",
            "Epoch 1 completed. Validation Loss: 0.9893474598725637\n",
            "Epoch 2 completed. Validation Loss: 1.06071029206117\n",
            "Epoch 3 completed. Validation Loss: 1.1581590235233308\n",
            "Stopping early due to overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction and Performance Evaluation\n",
        "\n",
        "After training the CamemBERT model, we proceed to evaluate its performance on the test set. This step is crucial for understanding how well the model generalizes to new, unseen data.\n",
        "\n",
        "First, we set the model to evaluation mode using `model.eval()`. This disables dropout and batch normalization layers for the inference phase.\n",
        "\n",
        "We then iterate over the test dataset using our DataLoader. For each batch, we perform a forward pass through the model without calculating gradients, as we are not training during this phase.\n",
        "\n",
        "The model outputs logits, which are then converted into predicted class labels using the `argmax` function. These predictions, along with the actual labels from the test data, are used to generate a classification report. This report provides a comprehensive overview of the model's performance, including metrics like precision, recall, and F1-score for each class.\n"
      ],
      "metadata": {
        "id": "7q6UNDBwaFLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Initialize lists to store predictions and actual labels\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "# Iterate over the test data\n",
        "for batch in test_loader:\n",
        "    # Move the batch to GPU for faster processing\n",
        "    batch = {k: v.to('cuda') for k, v in batch.items()}\n",
        "\n",
        "    # Disable gradient calculations as we are in inference mode\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)  # Forward pass with the batch\n",
        "\n",
        "    logits = outputs.logits  # Extract the logits from the model output\n",
        "    predictions.extend(logits.argmax(dim=1).tolist())  # Get the predicted labels\n",
        "    true_labels.extend(batch['labels'].tolist())  # Append the actual labels\n",
        "\n",
        "# Print the classification report comparing predictions and true labels\n",
        "print(classification_report(true_labels, predictions))\n"
      ],
      "metadata": {
        "id": "x3PyOW3CkXDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9563d65-989e-416d-897b-c5b7fabad3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.74      0.75        91\n",
            "           1       0.50      0.44      0.47        72\n",
            "           2       0.61      0.50      0.55        92\n",
            "           3       0.46      0.71      0.56        70\n",
            "           4       0.55      0.49      0.52        71\n",
            "           5       0.68      0.65      0.67        84\n",
            "\n",
            "    accuracy                           0.59       480\n",
            "   macro avg       0.59      0.59      0.59       480\n",
            "weighted avg       0.61      0.59      0.59       480\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Report Analysis\n",
        "\n",
        "#### Key Metrics:\n",
        "- **Precision**: Measures the ratio of correctly predicted positive observations to the total predicted positives. High precision relates to a low false positive rate.\n",
        "  - Example: Class 0 has a precision of 0.77, meaning 77% of instances predicted as class 0 are correctly identified.\n",
        "\n",
        "- **Recall (Sensitivity)**: Indicates the proportion of actual positives correctly identified.\n",
        "  - Example: Class 3 has a recall of 0.71, meaning the model correctly identifies 71% of all actual class 3 instances.\n",
        "\n",
        "- **F1-Score**: A weighted average of Precision and Recall, useful in imbalanced class distributions. A high F1 score implies low false positives and negatives.\n",
        "  - Example: Class 3 has the highest F1-score of 0.56 among the classes, indicating a balanced precision and recall.\n",
        "\n",
        "- **Support**: The actual number of occurrences of each class in the dataset.\n",
        "  - Example: Class 0 appears 91 times.\n",
        "\n",
        "- **Accuracy**: The overall rate of correctly predicted observations. The model's accuracy is 0.59 (59%), indicating its general effectiveness.\n",
        "\n",
        "- **Macro Avg**: The unweighted average of precision, recall, and F1-score across classes, standing at 0.59 for this model.\n",
        "\n",
        "- **Weighted Avg**: Accounts for class imbalance by weighting each class's metrics by its support, showing a slightly better performance at around 0.61.\n"
      ],
      "metadata": {
        "id": "hv51ppX8bc9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the Confusion Matrix\n",
        "\n",
        "A confusion matrix is a useful tool for understanding the performance of a classification model. It helps in visualizing the model's predictions in comparison with the actual labels.\n",
        "\n",
        "To create the confusion matrix, we use the `confusion_matrix` function from `sklearn.metrics`, passing in the true labels and the predictions made by the model.\n",
        "\n",
        "We then use Matplotlib and Seaborn libraries to plot the confusion matrix. `plt.figure(figsize=(10, 7))` sets the size of the plot for better visibility. `sns.heatmap` is used to create a heatmap representation of the confusion matrix. The parameters `annot=True` and `fmt='g'` ensure that the individual counts are displayed in each cell of the\n"
      ],
      "metadata": {
        "id": "HLJrz2i4aaiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "plt.figure(figsize=(10, 7))  # Set the figure size for the plot\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')  # Create a heatmap for the confusion matrix\n",
        "plt.xlabel('Predicted Labels')  # Label for the x-axis\n",
        "plt.ylabel('True Labels')  # Label for the y-axis\n",
        "plt.title('Confusion Matrix')  # Title for the plot\n",
        "plt.show()  # Display the plot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "O5WeCUjrLV9E",
        "outputId": "affed912-a5b7-45a3-8133-14b56d4c8d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analysis and Insights:\n",
        "- The model is most precise in predicting Class 0 and most sensitive in identifying Class 3.\n",
        "- The F1-score is fairly consistent across classes, with Class 3 achieving the highest score.\n",
        "- A moderate overall accuracy of 59% may be acceptable but also indicates potential areas for improvement.\n",
        "- Class imbalance (in quality maybe) is evident, impacting the macro and weighted averages.\n",
        "- Classes 1 and 2 show lower performance in precision and recall, suggesting areas for focused improvement.\n"
      ],
      "metadata": {
        "id": "PaqHHy7ragbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Retraining Model on Full Dataset\n",
        "\n",
        "After the initial training and evaluation phases, we now proceed to retrain the CamemBERT model on the entire dataset. This process is aimed at leveraging the full range of data available, enhancing the model's ability to generalize and perform effectively on real-world data.\n",
        "\n",
        "The steps for retraining include:\n",
        "\n",
        "1. **Initializing the CamemBERT Tokenizer**: We continue to use the `CamembertTokenizer` for consistent text preprocessing.\n",
        "\n",
        "2. **Preparing the Full Dataset**: The `FrenchTextDataset` is now initialized with the complete dataset, ensuring that all available text data and labels are included.\n",
        "\n",
        "3. **Data Loader Creation**: A DataLoader is set up for the full dataset, with an appropriate batch size and shuffling enabled to ensure diverse mini-batch training.\n",
        "\n",
        "4. **Training Loop**: The model undergoes training for a specified number of epochs. In each epoch, it performs the following steps on each batch of data:\n",
        "    - Forward pass: Processes the input data through the model.\n",
        "    - Loss Computation: Calculates the loss using the `CrossEntropyLoss` function with class weights.\n",
        "    - Backward Pass: Performs backpropagation to update the model's weights.\n",
        "    - Optimization Step: Applies gradient descent to optimize the model's parameters.\n",
        "\n",
        "By retraining on the full dataset, we aim to enhance the model's performance and prepare it for deployment in practical applications.\n"
      ],
      "metadata": {
        "id": "33P8uYIF2jtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize CamemBERT tokenizer\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert/camembert-base-ccnet')\n",
        "\n",
        "full_dataset = FrenchTextDataset(\n",
        "    texts=df['sentence'].to_numpy(),\n",
        "    labels=df['difficulty_encoded'].to_numpy(),\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "loader = DataLoader(full_dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "iqihzYnC2lvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):  # Adjust the number of epochs as needed\n",
        "    model.train()\n",
        "    for batch in loader:\n",
        "        input_ids = batch['input_ids'].to('cuda')\n",
        "        attention_mask = batch['attention_mask'].to('cuda')\n",
        "        labels = batch['labels'].to('cuda')\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(outputs.logits, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "uQyLQpyr3Ch9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 11: Preparing the Unlabeled Data\n",
        "\n",
        "In practical applications, we often deal with unlabeled data. To make predictions on such data using our trained CamemBERT model, we need to prepare it in a format that the model can process. The following steps are taken to achieve this:\n",
        "\n",
        "1. **Custom Dataset for Unlabeled Data**: We define a class `UnlabeledFrenchTextDataset`, similar to our labeled dataset class but tailored for unlabeled texts. It handles the tokenization and preparation of the texts without needing labels.\n",
        "\n",
        "2. **Initializing the Unlabeled Dataset**: We initialize this dataset with our unlabeled texts and the previously used tokenizer. This ensures consistency in text processing.\n",
        "\n",
        "3. **DataLoader for Unlabeled Data**: A DataLoader is created for the unlabeled dataset. This DataLoader will enable efficient and manageable processing of the data in batches, which is particularly useful when dealing with large amounts of data.\n",
        "\n",
        "By preparing the unlabeled data in this way, we ensure that it is in the correct format for the CamemBERT model to make predictions. This step is crucial for applying the model to real-world scenarios where labeled data might not always be available.\n"
      ],
      "metadata": {
        "id": "-TfbzV5_dMqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class for handling unlabeled French text data.\n",
        "class UnlabeledFrenchTextDataset(Dataset):\n",
        "\n",
        "    # Constructor to initialize the dataset.\n",
        "    # - `texts`: array of text data.\n",
        "    # - `tokenizer`: tokenizer used for encoding the texts.\n",
        "    # - `max_token_len`: maximum length of tokens for each text.\n",
        "    def __init__(self, texts, tokenizer, max_token_len=512):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_token_len = max_token_len\n",
        "\n",
        "    # Returns the total number of texts in the dataset.\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    # Retrieves a single item from the dataset by index.\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]  # Get the text at the specified index.\n",
        "\n",
        "        # Encode the text into a format suitable for the model.\n",
        "        # - Adds special tokens (like [CLS], [SEP]).\n",
        "        # - Pads or truncates the text to `max_token_len`.\n",
        "        # - Generates an attention mask for the input.\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Returns a dictionary with encoded inputs and attention mask.\n",
        "        # These are needed for model inference.\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }\n",
        "\n",
        "# Create an instance of the UnlabeledFrenchTextDataset class.\n",
        "# This dataset is used for processing the unlabeled texts.\n",
        "unlabeled_dataset = UnlabeledFrenchTextDataset(\n",
        "    texts=unlabeled_df['sentence'].to_numpy(),  # Convert the sentences from DataFrame to numpy array.\n",
        "    tokenizer=tokenizer  # Use the same tokenizer as in the training phase.\n",
        ")\n",
        "\n",
        "# DataLoader to efficiently handle batches of the unlabeled data.\n",
        "# - `batch_size`: Number of samples per batch to load.\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=16)\n",
        "\n"
      ],
      "metadata": {
        "id": "9GksBa8XrUYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 12: Predicting Unlabeled Data\n",
        "\n",
        "#### Evaluation Mode\n",
        "Firstly, we set our model to evaluation mode using `model.eval()`. This step is crucial as it prepares the model for inference, disabling certain layers like Dropout and BatchNorm that are only active during training.\n",
        "\n",
        "#### Making Predictions\n",
        "We then process the unlabeled data batch-wise. For each batch:\n",
        "- We transfer the batch data to CUDA for accelerated computation.\n",
        "- We use a `torch.no_grad()` context to perform inference without computing gradients, which is more memory efficient and faster.\n",
        "- The model's forward pass is computed with the batch data, yielding logits.\n",
        "\n",
        "#### Processing the Output\n",
        "- We apply `argmax` to the logits to get the most likely class label predictions.\n",
        "- These predictions are collected in a list, `predictions`.\n",
        "\n",
        "#### Label Conversion\n",
        "Finally, we convert our numeric predictions back to their original label format using the `LabelEncoder`'s `inverse_transform` method. This step provides us with the class labels in a human-readable form.\n"
      ],
      "metadata": {
        "id": "DmtGGQj-ebvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Switching the model to evaluation mode. This is essential as it disables\n",
        "# layers like Dropout and BatchNorm, which behave differently during training.\n",
        "model.eval()\n",
        "\n",
        "# List to store the predictions.\n",
        "predictions = []\n",
        "\n",
        "# Iterate over each batch in the DataLoader.\n",
        "for batch in unlabeled_loader:\n",
        "    # Move the batch data to the GPU for faster computation.\n",
        "    batch = {k: v.to('cuda') for k, v in batch.items()}\n",
        "\n",
        "    # Disable gradient calculations. This reduces memory usage and speeds up computation.\n",
        "    with torch.no_grad():\n",
        "        # Forward pass through the model.\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    # Extract the logits (raw model outputs prior to activation function) from the model outputs.\n",
        "    logits = outputs.logits\n",
        "    # Find the maximum logit to get the most likely class label for each instance in the batch.\n",
        "    preds = logits.argmax(dim=1).tolist()\n",
        "    # Extend the main predictions list with the batch predictions.\n",
        "    predictions.extend(preds)\n",
        "\n",
        "# Convert the numerical predictions back to their original categorical label form.\n",
        "predicted_labels = label_encoder.inverse_transform(predictions)\n",
        "\n"
      ],
      "metadata": {
        "id": "spHTv6oasBCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 13: Preparing Submission File for Kaggle\n",
        "\n",
        "#### Creating the Submission DataFrame\n",
        "We prepare a DataFrame named `submission_df` for submission. This DataFrame includes:\n",
        "- `id`: The unique identifiers from our `unlabeled_df`.\n",
        "- `difficulty`: The predicted labels obtained from the model for each text.\n",
        "\n",
        "#### Saving to CSV\n",
        "The DataFrame is then saved to a CSV file using the `to_csv` method. The filename 'submission_v39_Camembert-baseCCnet-epoch4-batch16-lr5e-5-weights-split01.csv' reflects the model version, training details, and the data split used. By setting `index=False`, we ensure that the DataFrame's index is not included in the CSV file, keeping only the relevant columns (`id` and `difficulty`).\n",
        "\n",
        "This step finalizes the process by creating a ready-to-submit file containing our model's predictions.\n"
      ],
      "metadata": {
        "id": "AGGQELkKemRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': unlabeled_df['id'],\n",
        "    'difficulty': predicted_labels\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "submission_file = 'submission_v39_Camembert-baseCCnet-epoch4-batch16-lr5e-5-weights-split01.csv'\n",
        "submission_df.to_csv(submission_file, index=False)"
      ],
      "metadata": {
        "id": "88h7WUQmsPgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 14: Saving the Trained Model\n",
        "\n",
        "#### Model Save Path\n",
        "We define the path where the trained model will be saved. In this case, it's specified as `\"/content/modelforApp.pth\"`. This path points to a location in the Colab environment, typically in the content directory. The `.pth` extension is commonly used for PyTorch model files.\n",
        "\n",
        "#### Saving the Model's State\n",
        "We use `torch.save` to save the model's state dictionary (`state_dict`). The state dictionary contains all the parameters of the model, such as the weights and biases of the neural network layers. Saving the state dictionary enables us to reload the model later with these exact parameters.\n",
        "\n",
        "#### Confirmation Message\n",
        "After saving the model, a confirmation message is printed, indicating the successful save operation and the path where the model is stored. This is helpful for verification and for locating the saved model file for future use.\n",
        "\n"
      ],
      "metadata": {
        "id": "hudMxOXdfLNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/modelforApp.pth\"\n",
        "\n",
        "# Save the model's state_dict\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "print(f\"Model saved successfully at {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qam7xDb2gaB1",
        "outputId": "138877c6-0efb-458c-aa07-66c2cffb11e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully at /content/modelforApp.pth\n"
          ]
        }
      ]
    }
  ]
}